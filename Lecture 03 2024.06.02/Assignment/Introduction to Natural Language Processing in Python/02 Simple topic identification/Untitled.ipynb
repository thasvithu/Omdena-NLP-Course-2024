{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6843a0a2",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words\n",
    "- Bag of Words (BoW) is a imple and powerful technique for converting text into numerical features that can be used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5702cd",
   "metadata": {},
   "source": [
    "![q1.png](q1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78d4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat is in the box. The cat box.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0059e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0bb16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'in', 'the', 'box', '.', 'the', 'cat', 'box', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b757e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'box', 'cat', 'in', 'is', 'the']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Vocabulary\n",
    "vocab = sorted(set(tokens))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efe2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d5cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 3, 'cat': 2, 'box': 2, '.': 2, 'is': 1, 'in': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting Frequencies\n",
    "word_freq = Counter(tokens)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414584f",
   "metadata": {},
   "source": [
    "#### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b69282",
   "metadata": {},
   "source": [
    "![q2.png](q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7331491",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c5f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3003356",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'Leading',\n",
       " 'AI',\n",
       " 'textbooks',\n",
       " 'define',\n",
       " 'the',\n",
       " 'field',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " ':',\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " '.',\n",
       " 'Colloquially',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'often',\n",
       " 'used',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'machines',\n",
       " '(',\n",
       " 'or',\n",
       " 'computers',\n",
       " ')',\n",
       " 'that',\n",
       " 'mimic',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'human',\n",
       " 'mind',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem-solving',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d8f581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'ai',\n",
       " ')',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'leading',\n",
       " 'ai',\n",
       " 'textbooks',\n",
       " 'define',\n",
       " 'the',\n",
       " 'field',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " ':',\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " '.',\n",
       " 'colloquially',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'often',\n",
       " 'used',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'machines',\n",
       " '(',\n",
       " 'or',\n",
       " 'computers',\n",
       " ')',\n",
       " 'that',\n",
       " 'mimic',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'human',\n",
       " 'mind',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem-solving',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6dac877",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 5,\n",
       "         '``': 5,\n",
       "         \"''\": 5,\n",
       "         'intelligence': 4,\n",
       "         'that': 4,\n",
       "         ',': 3,\n",
       "         'and': 3,\n",
       "         '.': 3,\n",
       "         'its': 3,\n",
       "         'artificial': 2,\n",
       "         '(': 2,\n",
       "         'ai': 2,\n",
       "         ')': 2,\n",
       "         'is': 2,\n",
       "         'by': 2,\n",
       "         'machines': 2,\n",
       "         'to': 2,\n",
       "         'humans': 2,\n",
       "         'as': 2,\n",
       "         'of': 2,\n",
       "         'demonstrated': 1,\n",
       "         'in': 1,\n",
       "         'contrast': 1,\n",
       "         'natural': 1,\n",
       "         'displayed': 1,\n",
       "         'animals': 1,\n",
       "         'leading': 1,\n",
       "         'textbooks': 1,\n",
       "         'define': 1,\n",
       "         'field': 1,\n",
       "         'study': 1,\n",
       "         'intelligent': 1,\n",
       "         'agents': 1,\n",
       "         ':': 1,\n",
       "         'any': 1,\n",
       "         'device': 1,\n",
       "         'perceives': 1,\n",
       "         'environment': 1,\n",
       "         'takes': 1,\n",
       "         'actions': 1,\n",
       "         'maximize': 1,\n",
       "         'chance': 1,\n",
       "         'successfully': 1,\n",
       "         'achieving': 1,\n",
       "         'goals': 1,\n",
       "         'colloquially': 1,\n",
       "         'term': 1,\n",
       "         'often': 1,\n",
       "         'used': 1,\n",
       "         'describe': 1,\n",
       "         'or': 1,\n",
       "         'computers': 1,\n",
       "         'mimic': 1,\n",
       "         'cognitive': 1,\n",
       "         'functions': 1,\n",
       "         'associate': 1,\n",
       "         'with': 1,\n",
       "         'human': 1,\n",
       "         'mind': 1,\n",
       "         'such': 1,\n",
       "         'learning': 1,\n",
       "         'problem-solving': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "bow_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "474334bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5), ('``', 5), (\"''\", 5), ('intelligence', 4), ('that', 4), (',', 3), ('and', 3), ('.', 3), ('its', 3), ('artificial', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0507e73",
   "metadata": {},
   "source": [
    "# Simple text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c2070",
   "metadata": {},
   "source": [
    "![q3.png](q4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ff53139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f0f203b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'for',\n",
       " 'testing',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'words',\n",
       " '.',\n",
       " 'The',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " '.',\n",
       " 'The',\n",
       " 'cat',\n",
       " 'box',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"This is an example text for testing bag of words. The cat is in the box. The cat box.\"\n",
    "tokens = word_tokenize(article)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddb6a998",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'for',\n",
       " 'testing',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'words',\n",
       " '.',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " '.',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'box',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fe0b057",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'for',\n",
       " 'testing',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'words',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'box']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out non-alphabetic tokens\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "alpha_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e33f3b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'text', 'testing', 'bag', 'words', 'cat', 'box', 'cat', 'box']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "english_stops = set(stopwords.words('english'))\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dca1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96aa63d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'text', 'testing', 'bag', 'word', 'cat', 'box', 'cat', 'box']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the tokens\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74f1f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'cat': 2,\n",
       "         'box': 2,\n",
       "         'example': 1,\n",
       "         'text': 1,\n",
       "         'testing': 1,\n",
       "         'bag': 1,\n",
       "         'word': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bag-of-words\n",
    "bow = Counter(lemmatized)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e4799e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('box', 2), ('example', 1), ('text', 1), ('testing', 1), ('bag', 1), ('word', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9eb12",
   "metadata": {},
   "source": [
    "# Introduction to gensim\n",
    "- Gensim is an open-source library designed for natural language processing (NLP), particularly focused on unsupervised topic modeling and natural language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a66e8",
   "metadata": {},
   "source": [
    "![q5.png](q5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "619dbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a092c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    [\"computer\", \"science\", \"study\", \"of\", \"computers\", \"and\", \"computational\", \"systems\"],\n",
    "    [\"artificial\", \"intelligence\", \"branch\", \"of\", \"computer\", \"science\", \"concerned\", \"with\", \"making\", \"computers\", \"behave\", \"like\", \"humans\"],\n",
    "    [\"machine\", \"learning\", \"subset\", \"of\", \"artificial\", \"intelligence\", \"focused\", \"on\", \"algorithms\", \"that\", \"allow\", \"computers\", \"to\", \"learn\", \"from\", \"data\"],\n",
    "    [\"neural\", \"networks\", \"computing\", \"systems\", \"inspired\", \"by\", \"the\", \"biological\", \"neural\", \"networks\", \"that\", \"constitute\", \"animal\", \"brains\"],\n",
    "    [\"data\", \"science\", \"interdisciplinary\", \"field\", \"that\", \"uses\", \"scientific\", \"methods\", \"processes\", \"algorithms\", \"and\", \"systems\", \"to\", \"extract\", \"knowledge\", \"and\", \"insights\", \"from\", \"data\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6827f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e18780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "computer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46898b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a4b6931",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(3, 1),\n",
       "  (4, 1),\n",
       "  (8, 1),\n",
       "  (13, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1)],\n",
       " [(7, 1),\n",
       "  (27, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 2),\n",
       "  (37, 2),\n",
       "  (38, 1)],\n",
       " [(0, 2),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (17, 1),\n",
       "  (19, 2),\n",
       "  (21, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b72f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (5, 1), (7, 1), (17, 1), (19, 2), (21, 1), (27, 1), (28, 1), (39, 1), (40, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5938a5",
   "metadata": {},
   "source": [
    "#### Gensim bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31884f1",
   "metadata": {},
   "source": [
    "![q6.png](q6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2967aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2be47233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28e11b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75578023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 2\n",
      "data 2\n",
      "science 1\n",
      "systems 1\n",
      "algorithms 1\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89c75163",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 3,\n",
       "             1: 1,\n",
       "             2: 2,\n",
       "             3: 3,\n",
       "             4: 3,\n",
       "             5: 3,\n",
       "             6: 1,\n",
       "             7: 3,\n",
       "             8: 2,\n",
       "             9: 1,\n",
       "             10: 1,\n",
       "             11: 1,\n",
       "             12: 1,\n",
       "             13: 2,\n",
       "             14: 1,\n",
       "             15: 1,\n",
       "             16: 1,\n",
       "             17: 2,\n",
       "             18: 1,\n",
       "             19: 3,\n",
       "             20: 1,\n",
       "             21: 2,\n",
       "             22: 1,\n",
       "             23: 1,\n",
       "             24: 1,\n",
       "             25: 1,\n",
       "             26: 1,\n",
       "             27: 3,\n",
       "             28: 2,\n",
       "             29: 1,\n",
       "             30: 1,\n",
       "             31: 1,\n",
       "             32: 1,\n",
       "             33: 1,\n",
       "             34: 1,\n",
       "             35: 1,\n",
       "             36: 2,\n",
       "             37: 2,\n",
       "             38: 1,\n",
       "             39: 1,\n",
       "             40: 1,\n",
       "             41: 1,\n",
       "             42: 1,\n",
       "             43: 1,\n",
       "             44: 1,\n",
       "             45: 1,\n",
       "             46: 1,\n",
       "             47: 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "total_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7ab7f",
   "metadata": {},
   "source": [
    "![q7.png](q7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31eb2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34d70454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be2fbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3af0ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 2\n",
      "data 2\n",
      "science 1\n",
      "systems 1\n",
      "algorithms 1\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6c3c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b618980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f2b3325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 3\n",
      "computers 3\n",
      "of 3\n",
      "science 3\n",
      "systems 3\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48e557",
   "metadata": {},
   "source": [
    "# Tf-idf with gensim\n",
    "- Term Frequency (TF): Measures how frequently a term occurs in a document. \n",
    "- Inverse Document Frequency (IDF): Measures how important a term is in the entire corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2e214",
   "metadata": {},
   "source": [
    "![q8.png](q8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3b5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1151292546497023"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "(5/100) * math.log(200/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae65d2",
   "metadata": {},
   "source": [
    "![q9.png](q9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7102dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.49169813431045906), (3, 0.49169813431045906), (4, 0.18147115159841573), (5, 0.49169813431045906), (6, 0.49169813431045906)]\n",
      "computers 0.49169813431045906\n",
      "of 0.49169813431045906\n",
      "science 0.18147115159841573\n",
      "study 0.49169813431045906\n",
      "the 0.49169813431045906\n"
     ]
    }
   ],
   "source": [
    "# Sample document and corpus for demonstration\n",
    "articles = [\n",
    "    ['computer', 'science', 'is', 'the', 'study', 'of', 'computers'],\n",
    "    ['computer', 'engineering', 'is', 'another', 'field'],\n",
    "    ['software', 'engineering', 'is', 'related', 'to', 'computer', 'science'],\n",
    "]\n",
    "\n",
    "# Import Dictionary from gensim.corpora.dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a Dictionary from the articles\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Create a corpus using the dictionary\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Example document for TF-IDF calculation\n",
    "doc = corpus[0]  # Using the first document in the corpus for this example\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Optional: Print the words along with their TF-IDF weights for better understanding\n",
    "for term_id, weight in tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac75e1",
   "metadata": {},
   "source": [
    "![q10.png](q10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3333c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.49169813431045906), (3, 0.49169813431045906), (4, 0.18147115159841573), (5, 0.49169813431045906), (6, 0.49169813431045906)]\n",
      "computers 0.49169813431045906\n",
      "of 0.49169813431045906\n",
      "study 0.49169813431045906\n",
      "the 0.49169813431045906\n",
      "science 0.18147115159841573\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d44d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
